flambo-example.zero=> (SparkStringConsumer/main (into-array String ["11"]))
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/08/12 09:30:56 INFO SparkContext: Running Spark version 2.2.0
17/08/12 09:30:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/08/12 09:31:01 INFO SparkContext: Submitted application: Consumer
17/08/12 09:31:01 INFO SecurityManager: Changing view acls to: clojure
17/08/12 09:31:01 INFO SecurityManager: Changing modify acls to: clojure
17/08/12 09:31:01 INFO SecurityManager: Changing view acls groups to:
17/08/12 09:31:01 INFO SecurityManager: Changing modify acls groups to:
17/08/12 09:31:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(clojure); groups with view permissions: Set(); users  with modify permissions: Set(clojure); groups with modify permissions: Set()
17/08/12 09:31:01 INFO Utils: Successfully started service 'sparkDriver' on port 53207.
17/08/12 09:31:01 INFO SparkEnv: Registering MapOutputTracker
17/08/12 09:31:01 INFO SparkEnv: Registering BlockManagerMaster
17/08/12 09:31:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/08/12 09:31:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/08/12 09:31:01 INFO DiskBlockManager: Created local directory at /private/var/folders/ft/xng6h3zs30d0md4nshw79m2h0000gn/T/blockmgr-ebbda5c7-cc5f-482a-9a1e-e7944f27f928
17/08/12 09:31:01 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
17/08/12 09:31:01 INFO SparkEnv: Registering OutputCommitCoordinator
17/08/12 09:31:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/08/12 09:31:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.100:4040
17/08/12 09:31:02 INFO Executor: Starting executor ID driver on host localhost
17/08/12 09:31:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53208.
17/08/12 09:31:02 INFO NettyBlockTransferService: Server created on 192.168.1.100:53208
17/08/12 09:31:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/08/12 09:31:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.100, 53208, None)
17/08/12 09:31:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.100:53208 with 912.3 MB RAM, BlockManagerId(driver, 192.168.1.100, 53208, None)
17/08/12 09:31:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.100, 53208, None)
17/08/12 09:31:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.100, 53208, None)
17/08/12 09:31:02 WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
17/08/12 09:31:02 INFO VerifiableProperties: Verifying properties
17/08/12 09:31:02 INFO VerifiableProperties: Property group.id is overridden to
17/08/12 09:31:02 INFO VerifiableProperties: Property zookeeper.connect is overridden to
17/08/12 09:31:02 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedChannelException
17/08/12 09:31:02 WARN StreamingContext: StreamingContext has not been started yet
17/08/12 09:31:02 INFO SparkUI: Stopped Spark web UI at http://192.168.1.100:4040
17/08/12 09:31:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/08/12 09:31:02 INFO MemoryStore: MemoryStore cleared
17/08/12 09:31:02 INFO BlockManager: BlockManager stopped
17/08/12 09:31:02 INFO BlockManagerMaster: BlockManagerMaster stopped
17/08/12 09:31:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/08/12 09:31:02 INFO SparkContext: Successfully stopped SparkContext
17/08/12 09:31:02 INFO SparkContext: SparkContext already stopped.

SparkException java.nio.channels.ClosedChannelException  org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply (KafkaCluster.scala:366)
flambo-example.zero=>
