flambo-example.zero=>  (SparkStringConsumer/main (into-array String [""]))
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/08/14 11:01:48 INFO SparkContext: Running Spark version 2.2.0
17/08/14 11:01:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/08/14 11:01:48 WARN Utils: Your hostname, marching-mbp resolves to a loopback address: 127.0.0.1; using 192.168.16.102 instead (on interface en0)
17/08/14 11:01:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/08/14 11:01:48 INFO SparkContext: Submitted application: Consumer
17/08/14 11:01:48 INFO SecurityManager: Changing view acls to: stevechan
17/08/14 11:01:48 INFO SecurityManager: Changing modify acls to: stevechan
17/08/14 11:01:48 INFO SecurityManager: Changing view acls groups to:
17/08/14 11:01:48 INFO SecurityManager: Changing modify acls groups to:
17/08/14 11:01:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(stevechan); groups with view permissions: Set(); users  with modify permissions: Set(stevechan); groups with modify permissions: Set()
17/08/14 11:01:49 INFO Utils: Successfully started service 'sparkDriver' on port 52712.
17/08/14 11:01:49 INFO SparkEnv: Registering MapOutputTracker
17/08/14 11:01:49 INFO SparkEnv: Registering BlockManagerMaster
17/08/14 11:01:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/08/14 11:01:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/08/14 11:01:49 INFO DiskBlockManager: Created local directory at /private/var/folders/9p/m3jr24_n6rx_pvzzd9zb1z7c0000gr/T/blockmgr-3d36b08d-c9a1-4d99-b7af-9958dbc958c7
17/08/14 11:01:49 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
17/08/14 11:01:49 INFO SparkEnv: Registering OutputCommitCoordinator
17/08/14 11:01:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/08/14 11:01:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.16.102:4040
17/08/14 11:01:49 INFO Executor: Starting executor ID driver on host localhost
17/08/14 11:01:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52713.
17/08/14 11:01:49 INFO NettyBlockTransferService: Server created on 192.168.16.102:52713
17/08/14 11:01:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/08/14 11:01:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.16.102, 52713, None)
17/08/14 11:01:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.16.102:52713 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.16.102, 52713, None)
17/08/14 11:01:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.16.102, 52713, None)
17/08/14 11:01:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.16.102, 52713, None)
17/08/14 11:01:49 INFO VerifiableProperties: Verifying properties
17/08/14 11:01:49 INFO VerifiableProperties: Property group.id is overridden to
17/08/14 11:01:49 INFO VerifiableProperties: Property zookeeper.connect is overridden to
17/08/14 11:01:50 INFO DirectKafkaInputDStream: Slide time = 1000 ms
17/08/14 11:01:50 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
17/08/14 11:01:50 INFO DirectKafkaInputDStream: Checkpoint interval = null
17/08/14 11:01:50 INFO DirectKafkaInputDStream: Remember interval = 1000 ms
17/08/14 11:01:50 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@157e38f2
17/08/14 11:01:50 INFO ForEachDStream: Slide time = 1000 ms
17/08/14 11:01:50 INFO ForEachDStream: Storage level = Serialized 1x Replicated
17/08/14 11:01:50 INFO ForEachDStream: Checkpoint interval = null
17/08/14 11:01:50 INFO ForEachDStream: Remember interval = 1000 ms
17/08/14 11:01:50 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@d4d538b
17/08/14 11:01:50 INFO RecurringTimer: Started timer for JobGenerator at time 1502679711000
17/08/14 11:01:50 INFO JobGenerator: Started JobGenerator at 1502679711000 ms
17/08/14 11:01:50 INFO JobScheduler: Started JobScheduler
17/08/14 11:01:50 INFO StreamingContext: StreamingContext started
17/08/14 11:01:51 INFO VerifiableProperties: Verifying properties
17/08/14 11:01:51 INFO VerifiableProperties: Property group.id is overridden to
17/08/14 11:01:51 INFO VerifiableProperties: Property zookeeper.connect is overridden to
17/08/14 11:01:51 INFO JobScheduler: Added jobs for time 1502679711000 ms
17/08/14 11:01:51 INFO JobScheduler: Starting job streaming job 1502679711000 ms.0 from job set of time 1502679711000 ms
17/08/14 11:01:51 INFO SparkContext: Starting job: foreach at SparkStringConsumer.java:36
17/08/14 11:01:51 INFO DAGScheduler: Got job 0 (foreach at SparkStringConsumer.java:36) with 1 output partitions
17/08/14 11:01:51 INFO DAGScheduler: Final stage: ResultStage 0 (foreach at SparkStringConsumer.java:36)
17/08/14 11:01:51 INFO DAGScheduler: Parents of final stage: List()
17/08/14 11:01:51 INFO DAGScheduler: Missing parents: List()
17/08/14 11:01:51 INFO DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at SparkStringConsumer.java:33), which has no missing parents
17/08/14 11:01:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.9 KB, free 2004.6 MB)
17/08/14 11:01:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1759.0 B, free 2004.6 MB)
17/08/14 11:01:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.16.102:52713 (size: 1759.0 B, free: 2004.6 MB)
17/08/14 11:01:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/08/14 11:01:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at SparkStringConsumer.java:33) (first 15 tasks are for partitions Vector(0))
17/08/14 11:01:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/08/14 11:01:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, NODE_LOCAL, 4742 bytes)
17/08/14 11:01:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/08/14 11:01:51 INFO KafkaRDD: Beginning offset 36 is the same as ending offset skipping w4u_messages 0
17/08/14 11:01:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
17/08/14 11:01:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 61 ms on localhost (executor driver) (1/1)
17/08/14 11:01:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
17/08/14 11:01:51 INFO DAGScheduler: ResultStage 0 (foreach at SparkStringConsumer.java:36) finished in 0.076 s
17/08/14 11:01:51 INFO DAGScheduler: Job 0 finished: foreach at SparkStringConsumer.java:36, took 0.219773 s
17/08/14 11:01:51 INFO JobScheduler: Finished job streaming job 1502679711000 ms.0 from job set of time 1502679711000 ms
17/08/14 11:01:51 INFO JobScheduler: Total delay: 0.325 s for time 1502679711000 ms (execution: 0.249 s)
17/08/14 11:01:51 INFO ReceivedBlockTracker: Deleting batches:
17/08/14 11:01:51 INFO InputInfoTracker: remove old batch metadata:
17/08/14 11:01:52 INFO JobScheduler: Added jobs for time 1502679712000 ms
17/08/14 11:01:52 INFO JobScheduler: Starting job streaming job 1502679712000 ms.0 from job set of time 1502679712000 ms
17/08/14 11:01:52 INFO SparkContext: Starting job: foreach at SparkStringConsumer.java:36
17/08/14 11:01:52 INFO DAGScheduler: Got job 1 (foreach at SparkStringConsumer.java:36) with 1 output partitions
17/08/14 11:01:52 INFO DAGScheduler: Final stage: ResultStage 1 (foreach at SparkStringConsumer.java:36)
17/08/14 11:01:52 INFO DAGScheduler: Parents of final stage: List()
17/08/14 11:01:52 INFO DAGScheduler: Missing parents: List()
17/08/14 11:01:52 INFO DAGScheduler: Submitting ResultStage 1 (KafkaRDD[1] at createDirectStream at SparkStringConsumer.java:33), which has no missing parents
17/08/14 11:01:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.9 KB, free 2004.6 MB)
17/08/14 11:01:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1760.0 B, free 2004.6 MB)
17/08/14 11:01:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.16.102:52713 (size: 1760.0 B, free: 2004.6 MB)
17/08/14 11:01:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
17/08/14 11:01:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (KafkaRDD[1] at createDirectStream at SparkStringConsumer.java:33) (first 15 tasks are for partitions Vector(0))
17/08/14 11:01:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/08/14 11:01:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, NODE_LOCAL, 4742 bytes)
17/08/14 11:01:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/08/14 11:01:52 INFO KafkaRDD: Beginning offset 36 is the same as ending offset skipping w4u_messages 0
17/08/14 11:01:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 708 bytes result sent to driver
17/08/14 11:01:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 7 ms on localhost (executor driver) (1/1)
17/08/14 11:01:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
17/08/14 11:01:52 INFO DAGScheduler: ResultStage 1 (foreach at SparkStringConsumer.java:36) finished in 0.009 s
17/08/14 11:01:52 INFO DAGScheduler: Job 1 finished: foreach at SparkStringConsumer.java:36, took 0.018095 s
17/08/14 11:01:52 INFO JobScheduler: Finished job streaming job 1502679712000 ms.0 from job set of time 1502679712000 ms
17/08/14 11:01:52 INFO JobScheduler: Total delay: 0.032 s for time 1502679712000 ms (execution: 0.024 s)
17/08/14 11:01:52 INFO KafkaRDD: Removing RDD 0 from persistence list
17/08/14 11:01:52 INFO ReceivedBlockTracker: Deleting batches:
17/08/14 11:01:52 INFO BlockManager: Removing RDD 0
17/08/14 11:01:52 INFO InputInfoTracker: remove old batch metadata:
17/08/14 11:01:53 INFO JobScheduler: Added jobs for time 1502679713000 ms
17/08/14 11:01:53 INFO JobScheduler: Starting job streaming job 1502679713000 ms.0 from job set of time 1502679713000 ms
17/08/14 11:01:53 INFO SparkContext: Starting job: foreach at SparkStringConsumer.java:36
17/08/14 11:01:53 INFO DAGScheduler: Got job 2 (foreach at SparkStringConsumer.java:36) with 1 output partitions
17/08/14 11:01:53 INFO DAGScheduler: Final stage: ResultStage 2 (foreach at SparkStringConsumer.java:36)
17/08/14 11:01:53 INFO DAGScheduler: Parents of final stage: List()
17/08/14 11:01:53 INFO DAGScheduler: Missing parents: List()
17/08/14 11:01:53 INFO DAGScheduler: Submitting ResultStage 2 (KafkaRDD[2] at createDirectStream at SparkStringConsumer.java:33), which has no missing parents
17/08/14 11:01:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 2004.6 MB)
17/08/14 11:01:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1761.0 B, free 2004.6 MB)
17/08/14 11:01:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.16.102:52713 (size: 1761.0 B, free: 2004.6 MB)
17/08/14 11:01:53 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/08/14 11:01:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (KafkaRDD[2] at createDirectStream at SparkStringConsumer.java:33) (first 15 tasks are for partitions Vector(0))
17/08/14 11:01:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/08/14 11:01:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, NODE_LOCAL, 4742 bytes)
17/08/14 11:01:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/08/14 11:01:53 INFO KafkaRDD: Beginning offset 36 is the same as ending offset skipping w4u_messages 0
17/08/14 11:01:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 708 bytes result sent to driver
17/08/14 11:01:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 6 ms on localhost (executor driver) (1/1)
17/08/14 11:01:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
17/08/14 11:01:53 INFO DAGScheduler: ResultStage 2 (foreach at SparkStringConsumer.java:36) finished in 0.007 s
17/08/14 11:01:53 INFO DAGScheduler: Job 2 finished: foreach at SparkStringConsumer.java:36, took 0.016105 s
17/08/14 11:01:53 INFO JobScheduler: Finished job streaming job 1502679713000 ms.0 from job set of time 1502679713000 ms
17/08/14 11:01:53 INFO JobScheduler: Total delay: 0.030 s for time 1502679713000 ms (execution: 0.023 s)
17/08/14 11:01:53 INFO KafkaRDD: Removing RDD 1 from persistence list
17/08/14 11:01:53 INFO BlockManager: Removing RDD 1
17/08/14 11:01:53 INFO ReceivedBlockTracker: Deleting batches:
17/08/14 11:01:53 INFO InputInfoTracker: remove old batch metadata: 1502679711000 ms
17/08/14 11:01:54 INFO JobScheduler: Added jobs for time 1502679714000 ms
17/08/14 11:01:54 INFO JobScheduler: Starting job streaming job 1502679714000 ms.0 from job set of time 1502679714000 ms
17/08/14 11:01:54 INFO SparkContext: Starting job: foreach at SparkStringConsumer.java:36
17/08/14 11:01:54 INFO DAGScheduler: Got job 3 (foreach at SparkStringConsumer.java:36) with 1 output partitions
17/08/14 11:01:54 INFO DAGScheduler: Final stage: ResultStage 3 (foreach at SparkStringConsumer.java:36)
17/08/14 11:01:54 INFO DAGScheduler: Parents of final stage: List()
17/08/14 11:01:54 INFO DAGScheduler: Missing parents: List()
17/08/14 11:01:54 INFO DAGScheduler: Submitting ResultStage 3 (KafkaRDD[3] at createDirectStream at SparkStringConsumer.java:33), which has no missing parents
17/08/14 11:01:54 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 2004.6 MB)
17/08/14 11:01:54 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1761.0 B, free 2004.6 MB)
17/08/14 11:01:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.16.102:52713 (size: 1761.0 B, free: 2004.6 MB)
17/08/14 11:01:54 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
17/08/14 11:01:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (KafkaRDD[3] at createDirectStream at SparkStringConsumer.java:33) (first 15 tasks are for partitions Vector(0))
17/08/14 11:01:54 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/08/14 11:01:54 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, NODE_LOCAL, 4742 bytes)
17/08/14 11:01:54 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/08/14 11:01:54 INFO KafkaRDD: Computing topic w4u_messages, partition 0 offsets 36 -> 37
17/08/14 11:01:54 INFO VerifiableProperties: Verifying properties
17/08/14 11:01:54 INFO VerifiableProperties: Property group.id is overridden to
17/08/14 11:01:54 INFO VerifiableProperties: Property zookeeper.connect is overridden to
----------null===========2332111111111111111
17/08/14 11:01:54 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 708 bytes result sent to driver
17/08/14 11:01:54 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 35 ms on localhost (executor driver) (1/1)
17/08/14 11:01:54 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
17/08/14 11:01:54 INFO DAGScheduler: ResultStage 3 (foreach at SparkStringConsumer.java:36) finished in 0.036 s
17/08/14 11:01:54 INFO DAGScheduler: Job 3 finished: foreach at SparkStringConsumer.java:36, took 0.043807 s
17/08/14 11:01:54 INFO JobScheduler: Finished job streaming job 1502679714000 ms.0 from job set of time 1502679714000 ms
17/08/14 11:01:54 INFO JobScheduler: Total delay: 0.055 s for time 1502679714000 ms (execution: 0.048 s)
17/08/14 11:01:54 INFO KafkaRDD: Removing RDD 2 from persistence list
17/08/14 11:01:54 INFO BlockManager: Removing RDD 2
17/08/14 11:01:54 INFO ReceivedBlockTracker: Deleting batches:
17/08/14 11:01:54 INFO InputInfoTracker: remove old batch metadata: 1502679712000 ms
17/08/14 11:01:55 INFO JobScheduler: Added jobs for time 1502679715000 ms
17/08/14 11:01:55 INFO JobScheduler: Starting job streaming job 1502679715000 ms.0 from job set of time 1502679715000 ms
17/08/14 11:01:55 INFO SparkContext: Starting job: foreach at SparkStringConsumer.java:36
17/08/14 11:01:55 INFO DAGScheduler: Got job 4 (foreach at SparkStringConsumer.java:36) with 1 output partitions
17/08/14 11:01:55 INFO DAGScheduler: Final stage: ResultStage 4 (foreach at SparkStringConsumer.java:36)
17/08/14 11:01:55 INFO DAGScheduler: Parents of final stage: List()
17/08/14 11:01:55 INFO DAGScheduler: Missing parents: List()
17/08/14 11:01:55 INFO DAGScheduler: Submitting ResultStage 4 (KafkaRDD[4] at createDirectStream at SparkStringConsumer.java:33), which has no missing parents
17/08/14 11:01:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 2.9 KB, free 2004.6 MB)
17/08/14 11:01:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1761.0 B, free 2004.6 MB)
17/08/14 11:01:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.16.102:52713 (size: 1761.0 B, free: 2004.6 MB)
17/08/14 11:01:55 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/08/14 11:01:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (KafkaRDD[4] at createDirectStream at SparkStringConsumer.java:33) (first 15 tasks are for partitions Vector(0))
17/08/14 11:01:55 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/08/14 11:01:55 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, NODE_LOCAL, 4742 bytes)
17/08/14 11:01:55 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
17/08/14 11:01:55 INFO KafkaRDD: Beginning offset 37 is the same as ending offset skipping w4u_messages 0
17/08/14 11:01:55 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
17/08/14 11:01:55 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 3 ms on localhost (executor driver) (1/1)
17/08/14 11:01:55 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
17/08/14 11:01:55 INFO DAGScheduler: ResultStage 4 (foreach at SparkStringConsumer.java:36) finished in 0.004 s
17/08/14 11:01:55 INFO DAGScheduler: Job 4 finished: foreach at SparkStringConsumer.java:36, took 0.009937 s
17/08/14 11:01:55 INFO JobScheduler: Finished job streaming job 1502679715000 ms.0 from job set of time 1502679715000 ms
17/08/14 11:01:55 INFO JobScheduler: Total delay: 0.020 s for time 1502679715000 ms (execution: 0.015 s)
17/08/14 11:01:55 INFO KafkaRDD: Removing RDD 3 from persistence list
17/08/14 11:01:55 INFO BlockManager: Removing RDD 3
17/08/14 11:01:55 INFO ReceivedBlockTracker: Deleting batches:
17/08/14 11:01:55 INFO InputInfoTracker: remove old batch metadata: 1502679713000 ms
17/08/14 11:01:56 INFO JobScheduler: Added jobs for time 1502679716000 ms
17/08/14 11:01:56 INFO JobScheduler: Starting job streaming job 1502679716000 ms.0 from job set of time 1502679716000 ms
17/08/14 11:01:56 INFO SparkContext: Starting job: foreach at SparkStringConsumer.java:36
17/08/14 11:01:56 INFO DAGScheduler: Got job 5 (foreach at SparkStringConsumer.java:36) with 1 output partitions
17/08/14 11:01:56 INFO DAGScheduler: Final stage: ResultStage 5 (foreach at SparkStringConsumer.java:36)
17/08/14 11:01:56 INFO DAGScheduler: Parents of final stage: List()
17/08/14 11:01:56 INFO DAGScheduler: Missing parents: List()
17/08/14 11:01:56 INFO DAGScheduler: Submitting ResultStage 5 (KafkaRDD[5] at createDirectStream at SparkStringConsumer.java:33), which has no missing parents
17/08/14 11:01:56 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 2.9 KB, free 2004.6 MB)
17/08/14 11:01:56 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1761.0 B, free 2004.6 MB)
17/08/14 11:01:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.16.102:52713 (size: 1761.0 B, free: 2004.6 MB)
17/08/14 11:01:56 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
17/08/14 11:01:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (KafkaRDD[5] at createDirectStream at SparkStringConsumer.java:33) (first 15 tasks are for partitions Vector(0))
17/08/14 11:01:56 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
17/08/14 11:01:56 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, NODE_LOCAL, 4742 bytes)
17/08/14 11:01:56 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
17/08/14 11:01:56 INFO KafkaRDD: Beginning offset 37 is the same as ending offset skipping w4u_messages 0
17/08/14 11:01:56 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 665 bytes result sent to driver
17/08/14 11:01:56 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 3 ms on localhost (executor driver) (1/1)
17/08/14 11:01:56 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
17/08/14 11:01:56 INFO DAGScheduler: ResultStage 5 (foreach at SparkStringConsumer.java:36) finished in 0.004 s
17/08/14 11:01:56 INFO DAGScheduler: Job 5 finished: foreach at SparkStringConsumer.java:36, took 0.009223 s
17/08/14 11:01:56 INFO JobScheduler: Finished job streaming job 1502679716000 ms.0 from job set of time 1502679716000 ms
17/08/14 11:01:56 INFO JobScheduler: Total delay: 0.016 s for time 1502679716000 ms (execution: 0.012 s)
17/08/14 11:01:56 INFO KafkaRDD: Removing RDD 4 from persistence list
17/08/14 11:01:56 INFO BlockManager: Removing RDD 4
17/08/14 11:01:56 INFO ReceivedBlockTracker: Deleting batches:
17/08/14 11:01:56 INFO InputInfoTracker: remove old batch metadata: 1502679714000 ms
17/08/14 11:01:57 INFO JobScheduler: Added jobs for time 1502679717000 ms
17/08/14 11:01:57 INFO JobScheduler: Starting job streaming job 1502679717000 ms.0 from job set of time 1502679717000 ms
17/08/14 11:01:57 INFO SparkContext: Starting job: foreach at SparkStringConsumer.java:36
17/08/14 11:01:57 INFO DAGScheduler: Got job 6 (foreach at SparkStringConsumer.java:36) with 1 output partitions
17/08/14 11:01:57 INFO DAGScheduler: Final stage: ResultStage 6 (foreach at SparkStringConsumer.java:36)
17/08/14 11:01:57 INFO DAGScheduler: Parents of final stage: List()
17/08/14 11:01:57 INFO DAGScheduler: Missing parents: List()
17/08/14 11:01:57 INFO DAGScheduler: Submitting ResultStage 6 (KafkaRDD[6] at createDirectStream at SparkStringConsumer.java:33), which has no missing parents
17/08/14 11:01:57 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.9 KB, free 2004.6 MB)
17/08/14 11:01:57 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1761.0 B, free 2004.6 MB)
17/08/14 11:01:57 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.16.102:52713 (size: 1761.0 B, free: 2004.6 MB)
17/08/14 11:01:57 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
17/08/14 11:01:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (KafkaRDD[6] at createDirectStream at SparkStringConsumer.java:33) (first 15 tasks are for partitions Vector(0))
17/08/14 11:01:57 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/08/14 11:01:57 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, NODE_LOCAL, 4742 bytes)
17/08/14 11:01:57 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
17/08/14 11:01:57 INFO KafkaRDD: Beginning offset 37 is the same as ending offset skipping w4u_messages 0
17/08/14 11:01:57 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 665 bytes result sent to driver
17/08/14 11:01:57 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 4 ms on localhost (executor driver) (1/1)
17/08/14 11:01:57 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
17/08/14 11:01:57 INFO DAGScheduler: ResultStage 6 (foreach at SparkStringConsumer.java:36) finished in 0.004 s
17/08/14 11:01:57 INFO DAGScheduler: Job 6 finished: foreach at SparkStringConsumer.java:36, took 0.009760 s
17/08/14 11:01:57 INFO JobScheduler: Finished job streaming job 1502679717000 ms.0 from job set of time 1502679717000 ms
17/08/14 11:01:57 INFO JobScheduler: Total delay: 0.019 s for time 1502679717000 ms (execution: 0.015 s)
17/08/14 11:01:57 INFO KafkaRDD: Removing RDD 5 from persistence list
17/08/14 11:01:57 INFO BlockManager: Removing RDD 5
17/08/14 11:01:57 INFO ReceivedBlockTracker: Deleting batches:
17/08/14 11:01:57 INFO InputInfoTracker: remove old batch metadata: 1502679715000 ms
17/08/14 11:01:58 INFO JobScheduler: Added jobs for time 1502679718000 ms
17/08/14 11:01:58 INFO JobScheduler: Starting job streaming job 1502679718000 ms.0 from job set of time 1502679718000 ms
17/08/14 11:01:58 INFO SparkContext: Starting job: foreach at SparkStringConsumer.java:36
17/08/14 11:01:58 INFO DAGScheduler: Got job 7 (foreach at SparkStringConsumer.java:36) with 1 output partitions
17/08/14 11:01:58 INFO DAGScheduler: Final stage: ResultStage 7 (foreach at SparkStringConsumer.java:36)
17/08/14 11:01:58 INFO DAGScheduler: Parents of final stage: List()
17/08/14 11:01:58 INFO DAGScheduler: Missing parents: List()
17/08/14 11:01:58 INFO DAGScheduler: Submitting ResultStage 7 (KafkaRDD[7] at createDirectStream at SparkStringConsumer.java:33), which has no missing parents
17/08/14 11:01:58 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 2.9 KB, free 2004.6 MB)
17/08/14 11:01:58 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 1761.0 B, free 2004.6 MB)
17/08/14 11:01:58 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.16.102:52713 (size: 1761.0 B, free: 2004.6 MB)
17/08/14 11:01:58 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
17/08/14 11:01:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (KafkaRDD[7] at createDirectStream at SparkStringConsumer.java:33) (first 15 tasks are for partitions Vector(0))
17/08/14 11:01:58 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
17/08/14 11:01:58 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, NODE_LOCAL, 4742 bytes)
17/08/14 11:01:58 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
17/08/14 11:01:58 INFO KafkaRDD: Computing topic w4u_messages, partition 0 offsets 37 -> 38
17/08/14 11:01:58 INFO VerifiableProperties: Verifying properties
17/08/14 11:01:58 INFO VerifiableProperties: Property group.id is overridden to
17/08/14 11:01:58 INFO VerifiableProperties: Property zookeeper.connect is overridden to
----------null===========3211111111111111111111111
17/08/14 11:01:58 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 665 bytes result sent to driver
17/08/14 11:01:58 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 6 ms on localhost (executor driver) (1/1)
17/08/14 11:01:58 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
17/08/14 11:01:58 INFO DAGScheduler: ResultStage 7 (foreach at SparkStringConsumer.java:36) finished in 0.006 s
17/08/14 11:01:58 INFO DAGScheduler: Job 7 finished: foreach at SparkStringConsumer.java:36, took 0.012184 s
17/08/14 11:01:58 INFO JobScheduler: Finished job streaming job 1502679718000 ms.0 from job set of time 1502679718000 ms
17/08/14 11:01:58 INFO JobScheduler: Total delay: 0.021 s for time 1502679718000 ms (execution: 0.016 s)
17/08/14 11:01:58 INFO KafkaRDD: Removing RDD 6 from persistence list
17/08/14 11:01:58 INFO BlockManager: Removing RDD 6
17/08/14 11:01:58 INFO ReceivedBlockTracker: Deleting batches:
17/08/14 11:01:58 INFO InputInfoTracker: remove old batch metadata: 1502679716000 ms
17/08/14 11:01:59 INFO JobScheduler: Added jobs for time 1502679719000 ms
17/08/14 11:01:59 INFO JobScheduler: Starting job streaming job 1502679719000 ms.0 from job set of time 1502679719000 ms
17/08/14 11:01:59 INFO SparkContext: Starting job: foreach at SparkStringConsumer.java:36
17/08/14 11:01:59 INFO DAGScheduler: Got job 8 (foreach at SparkStringConsumer.java:36) with 1 output partitions
17/08/14 11:01:59 INFO DAGScheduler: Final stage: ResultStage 8 (foreach at SparkStringConsumer.java:36)
17/08/14 11:01:59 INFO DAGScheduler: Parents of final stage: List()
17/08/14 11:01:59 INFO DAGScheduler: Missing parents: List()
17/08/14 11:01:59 INFO DAGScheduler: Submitting ResultStage 8 (KafkaRDD[8] at createDirectStream at SparkStringConsumer.java:33), which has no missing parents
17/08/14 11:01:59 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 2.9 KB, free 2004.6 MB)
17/08/14 11:01:59 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 1761.0 B, free 2004.6 MB)
17/08/14 11:01:59 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.16.102:52713 (size: 1761.0 B, free: 2004.6 MB)
17/08/14 11:01:59 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/08/14 11:01:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (KafkaRDD[8] at createDirectStream at SparkStringConsumer.java:33) (first 15 tasks are for partitions Vector(0))
17/08/14 11:01:59 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/08/14 11:01:59 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, NODE_LOCAL, 4742 bytes)
17/08/14 11:01:59 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
17/08/14 11:01:59 INFO KafkaRDD: Beginning offset 38 is the same as ending offset skipping w4u_messages 0
17/08/14 11:01:59 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 665 bytes result sent to driver
17/08/14 11:01:59 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 4 ms on localhost (executor driver) (1/1)
17/08/14 11:01:59 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
17/08/14 11:01:59 INFO DAGScheduler: ResultStage 8 (foreach at SparkStringConsumer.java:36) finished in 0.004 s
17/08/14 11:01:59 INFO DAGScheduler: Job 8 finished: foreach at SparkStringConsumer.java:36, took 0.010073 s
17/08/14 11:01:59 INFO JobScheduler: Finished job streaming job 1502679719000 ms.0 from job set of time 1502679719000 ms
17/08/14 11:01:59 INFO JobScheduler: Total delay: 0.019 s for time 1502679719000 ms (execution: 0.013 s)
17/08/14 11:01:59 INFO KafkaRDD: Removing RDD 7 from persistence list
17/08/14 11:01:59 INFO BlockManager: Removing RDD 7
17/08/14 11:01:59 INFO ReceivedBlockTracker: Deleting batches:
17/08/14 11:01:59 INFO InputInfoTracker: remove old batch metadata: 1502679717000 ms
17/08/14 11:02:00 INFO JobScheduler: Added jobs for time 1502679720000 ms
17/08/14 11:02:00 INFO JobScheduler: Starting job streaming job 1502679720000 ms.0 from job set of time 1502679720000 ms
17/08/14 11:02:00 INFO SparkContext: Starting job: foreach at SparkStringConsumer.java:36
17/08/14 11:02:00 INFO DAGScheduler: Got job 9 (foreach at SparkStringConsumer.java:36) with 1 output partitions
17/08/14 11:02:00 INFO DAGScheduler: Final stage: ResultStage 9 (foreach at SparkStringConsumer.java:36)
17/08/14 11:02:00 INFO DAGScheduler: Parents of final stage: List()
17/08/14 11:02:00 INFO DAGScheduler: Missing parents: List()
17/08/14 11:02:00 INFO DAGScheduler: Submitting ResultStage 9 (KafkaRDD[9] at createDirectStream at SparkStringConsumer.java:33), which has no missing parents
17/08/14 11:02:00 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 2.9 KB, free 2004.6 MB)
17/08/14 11:02:00 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1761.0 B, free 2004.6 MB)
17/08/14 11:02:00 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.16.102:52713 (size: 1761.0 B, free: 2004.6 MB)
17/08/14 11:02:00 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/08/14 11:02:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (KafkaRDD[9] at createDirectStream at SparkStringConsumer.java:33) (first 15 tasks are for partitions Vector(0))
17/08/14 11:02:00 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
17/08/14 11:02:00 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, NODE_LOCAL, 4742 bytes)
17/08/14 11:02:00 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
17/08/14 11:02:00 INFO KafkaRDD: Beginning offset 38 is the same as ending offset skipping w4u_messages 0
17/08/14 11:02:00 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 665 bytes result sent to driver
17/08/14 11:02:00 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 2 ms on localhost (executor driver) (1/1)
17/08/14 11:02:00 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
17/08/14 11:02:00 INFO DAGScheduler: ResultStage 9 (foreach at SparkStringConsumer.java:36) finished in 0.003 s
17/08/14 11:02:00 INFO DAGScheduler: Job 9 finished: foreach at SparkStringConsumer.java:36, took 0.008516 s
17/08/14 11:02:00 INFO JobScheduler: Finished job streaming job 1502679720000 ms.0 from job set of time 1502679720000 ms
17/08/14 11:02:00 INFO KafkaRDD: Removing RDD 8 from persistence list
17/08/14 11:02:00 INFO JobScheduler: Total delay: 0.016 s for time 1502679720000 ms (execution: 0.012 s)
17/08/14 11:02:00 INFO BlockManager: Removing RDD 8
17/08/14 11:02:00 INFO ReceivedBlockTracker: Deleting batches:
17/08/14 11:02:00 INFO InputInfoTracker: remove old batch metadata: 1502679718000 ms

17/08/14 11:02:00 INFO ReceiverTracker: ReceiverTracker stopped
17/08/14 11:02:00 INFO JobGenerator: Stopping JobGenerator immediately
17/08/14 11:02:00 INFO SparkUI: Stopped Spark web UI at http://192.168.16.102:4040
17/08/14 11:02:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/08/14 11:02:00 INFO MemoryStore: MemoryStore cleared
17/08/14 11:02:00 INFO BlockManager: BlockManager stopped
17/08/14 11:02:00 INFO BlockManagerMaster: BlockManagerMaster stopped
flambo-example.zero=> 17/08/14 11:02:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/08/14 11:02:00 INFO SparkContext: Successfully stopped SparkContext
IllegalMonitorStateException   java.util.concurrent.locks.ReentrantLock$Sync.tryRelease (ReentrantLock.java:151)


flambo-example.zero=>
