(ns flambo-example.socket
  (:require [flambo.conf :as conf]
            [flambo.api :as api]
            [flambo.function :as function]
            [flambo.streaming :as streaming]
            [clojure.tools.logging :as log])
  (:import [org.apache.spark.streaming.api.java JavaStreamingContext]
           [org.apache.spark.streaming.receiver Receiver]
           [org.apache.spark.storage StorageLevel]
           [org.apache.kafka.common.serialization StringSerializer]
           [org.apache.kafka.clients.producer KafkaProducer ProducerRecord Callback RecordMetadata]
           [java.util Map]
           [java.io PrintWriter]
           [java.net ServerSocket]
           ;; mllib
           [breeze.linalg DenseVector]
           [org.apache.spark.mllib.linalg Vectors]
           [org.apache.spark.mllib.regression StreamingLinearRegressionWithSGD LabeledPoint]
           [org.apache.spark.streaming Seconds StreamingContext])
  (:gen-class))

(defn num-range-receiver
  [n]
  (proxy [Receiver] [(StorageLevel/MEMORY_AND_DISK_2)]
    (onStart []
      (require '[clojure.tools.logging :as log])
      (log/info "Starting receiver")
      (future
        (doseq [x (range 1 n)]
          (log/info (str "Store: " x))
          (.store this x)
          (Thread/sleep (rand-int 500)))))
    (onStop [] (log/info "Stopping receiver"))))

(api/defsparkfn fizzbuzz [x]
  (let [r (cond
            (zero? (mod x 15)) "FizzBuzz"
            (zero? (mod x 5)) "Buzz"
            (zero? (mod x 3)) "Fizz"
            :else x)]
    (str [x r])))

(api/defsparkfn publish [rdd _]
  (doseq [x (.collect rdd)]
    (log/info (str "====>>> Sending to Kafka fizzbuzz " x " , " (type x)))
    #_(send! @producer "fizzbuzz" x (->callback x)) ))

(api/defsparkfn publish-using-partitions [rdd _]
  (.foreachPartition
   rdd
   (function/void-function
    (api/fn [xs]
      (doseq [x (iterator-seq xs)]
        (log/info (str "---->>> Sending to Kafka fizzbuzz " x " , " (type x) ))
        #_(send! (memoized-producer producer-config) "fizzbuzz" x (->callback x)))))))

(def env {"spark.executor.memory" "1G"
          "spark.files.overwrite" "true"})

(def c (-> (conf/spark-conf)
           (conf/master "local[*]")
           (conf/app-name "flambo-custom-receiver-kafka-eample")
           (conf/set "spark.akka.timeout" "1000")
           (conf/set-executor-env env)))

#_(defn -main [& [n]]
  (log/info "Starting!")
  (let [ssc (streaming/streaming-context c 10000)
        stream (.receiverStream ^JavaStreamingContext ssc (num-range-receiver (Integer/parseInt n)))]
    (-> stream
        (streaming/map fizzbuzz)
        (streaming/foreach-rdd publish-using-partitions))
    (.start ssc)
    (.awaitTerminationOrTimeout ssc 90000)))

;; ------------>>> comment --->>>>>>>
(def ssc (streaming/streaming-context c 10000))

(def stream (.socketTextStream ssc "localhost" 9999))

(def zero-data (into-array Double (repeat 100 0.0)))

;; 大的流程打通了,小的细节用Clojure直接分析就行了 + R 语言
#_(def model
  (doto (StreamingLinearRegressionWithSGD.)
    (.setInitialWeights (Vectors/dense 0.1 #_zero-data))
    (.setNumIterations 1)
    (.setStepSize 0.01)))

(defn -main [& [n]]
  (log/info "Starting!")
  )

#_(streaming/map stream (fn [event] (log/info event)))

#_(api/defsparkfn fizzbuzz [x]
  (log/info x))

(api/defsparkfn fizzbuzz [x]
  (let [r (cond
            (zero? (mod x 15)) "FizzBuzz"
            (zero? (mod x 5)) "Buzz"
            (zero? (mod x 3)) "Fizz"
            :else x)]
    (str [x r])))

(-> stream
    (streaming/map fizzbuzz))

(do
  (.start ssc)
  (.awaitTermination ssc)
  )

;;     // create a stream of labeled points
;;     val labeledStream = stream.map { event =>
;;       val split = event.split("\t")
;;       val y = split(0).toDouble
;;       val features = split(1).split(",").map(_.toDouble)
;;       LabeledPoint(label = y, features = Vectors.dense(features))
;;     }
;; 
;;     // train and test model on the stream, and print predictions for illustrative purposes
;;     model.trainOn(labeledStream)
;;     //model.predictOn(labeledStream).print()
;;     model.predictOnValues(labeledStream.map(lp => (lp.label, lp.features))).print()
;; 
;;     ssc.start()
;;     ssc.awaitTermination()
;; 
